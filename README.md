# ğŸš€ Projeto de Engenharia de Dados: IngestÃ£o Automatizada de Indicadores PÃºblicos (IBGE e BCB)

## ğŸ“Œ VisÃ£o Geral
Este projeto visa automatizar a coleta, transformaÃ§Ã£o e carga de dados pÃºblicos dos principais indicadores econÃ´micos e sociais fornecidos por **IBGE** e **Banco Central do Brasil (BCB)**. Os dados sÃ£o estruturados e armazenados no **Google BigQuery**, prontos para anÃ¡lises e visualizaÃ§Ãµes avanÃ§adas.

A soluÃ§Ã£o foi desenvolvida com foco em **modularidade, boas prÃ¡ticas de engenharia de dados** e integraÃ§Ã£o com o **Google Cloud Platform**, utilizando **Python**, **Airflow** e **Cloud Composer**.

## ğŸ¯ Objetivos
- Automatizar a ingestÃ£o de dados via APIs pÃºblicas (IBGE e BCB).
- Transformar dados brutos em formato analÃ­tico utilizando `pandas`.
- Armazenar os dados em tabelas particionadas e otimizadas no BigQuery.
- Agendar execuÃ§Ãµes com o Airflow via Cloud Composer.
- Garantir **idempotÃªncia**, **reusabilidade** e **observabilidade** dos pipelines.

## âš™ï¸ Tecnologias e Bibliotecas
- Python 3.10+
- Apache Airflow (via Cloud Composer)
- Google BigQuery
- Google Cloud Platform (GCP)
- `requests`, `pandas`
- `google-cloud-bigquery`
- `apache-airflow-providers-google`
- `python-dotenv` (execuÃ§Ã£o local)

## ğŸ“ Estrutura do Projeto
```plaintext
ingestao_dados_publicos/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ gcp_credentials.json         # Chave da Service Account
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ ibge_pipeline_dag.py         # DAG Airflow - IBGE
â”‚   â”œâ”€â”€ bcb_pipeline_dag.py          # DAG Airflow - BCB
â”œâ”€â”€ src/                         # CÃ³digo-fonte dos pipelines
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ bigquery_operations.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ ibge_pipeline/
â”‚   â”‚   â”œâ”€â”€ extractor.py
â”‚   â”‚   â”œâ”€â”€ transformer.py
â”‚   â”‚   â””â”€â”€ main_ibge.py
â”‚   â””â”€â”€ bcb_pipeline/
â”‚       â”œâ”€â”€ extractor.py
â”‚       â”œâ”€â”€ transformer.py
â”‚       â””â”€â”€ main_bcb.py
â”œâ”€â”€ notebooks/                       # AnÃ¡lises exploratÃ³rias (opcional)
â”œâ”€â”€ tests/                           # Testes unitÃ¡rios
â”œâ”€â”€ .env                             # VariÃ¡veis de ambiente local
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o do Ambiente Local
1. **Clonar o repositÃ³rio**
   ```bash
   git clone https://github.com/HygorSX/proj-ingestao-dados.git
   cd ingestao_dados_publicos
   ```

2. **Criar ambiente virtual e instalar dependÃªncias**
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

3. **Criar arquivo .env**
   ```ini
   # .env
   GCP_PROJECT_ID=seu-projeto-gcp
   BIGQUERY_DATASET_IBGE=dados_publicos_ibge
   BIGQUERY_DATASET_BCB=dados_publicos_bcb
   GCP_LOCATION=southamerica-east1
   GOOGLE_APPLICATION_CREDENTIALS=./config/gcp_credentials.json
   ```

## â˜ï¸ ConfiguraÃ§Ã£o no Google Cloud
1. **Criar Projeto GCP e ativar APIs**
   - BigQuery API
   - Cloud Composer API

2. **Criar Service Account**
   - PermissÃµes: BigQuery Data Editor, BigQuery Job User, Storage Object Admin, Composer Worker

3. **Criar ambiente no Cloud Composer**
   - VersÃ£o recomendada: Composer 2.x / Airflow 2.x
   - RegiÃ£o: southamerica-east1

   Suba a pasta `dags/` e `src/` para o bucket:
   ```bash
   gsutil -m cp -r dags/ gs://<seu-bucket-composer>/dags/
   ```

4. **Configurar VariÃ¡veis na UI do Airflow (Admin > Variables)**
   | Key                  | Value                      |
   |----------------------|----------------------------|
   | GCP_PROJECT_ID       | ingestao-dados-publicos    |
   | BIGQUERY_DATASET_IBGE| dados_publicos_ibge        |
   | BIGQUERY_DATASET_BCB | dados_publicos_bcb         |
   | GCP_LOCATION         | southamerica-east1         |

## ğŸ”„ Pipelines Desenvolvidos
### ğŸ“Š IBGE
Indicadores tratados:
- IPCA - VariaÃ§Ã£o Mensal
- Taxa de DesocupaÃ§Ã£o
- PIB Anual
- PopulaÃ§Ã£o Estimada

### ğŸ’° BCB
SÃ©ries econÃ´micas:
- SELIC (vÃ¡rias variantes)
- DÃ³lar e Euro (PTAX)
- InadimplÃªncia de crÃ©dito
- Saldo de crÃ©dito para PF

**Etapas do ETL**
- **ExtraÃ§Ã£o**: APIs pÃºblicas (`requests`)
- **TransformaÃ§Ã£o**: `pandas`, tratamento de datas, valores nulos, tipos
- **Carga**: Tabela staging + MERGE â†’ Tabela final no BigQuery

## â±ï¸ Agendamento e ExecuÃ§Ã£o com Airflow
As DAGs foram criadas com:
- Agendamento diÃ¡rio (`@daily`)
- Logging centralizado com `setup_logging`
- FunÃ§Ãµes Python modulares
- Retry com delay padrÃ£o

```python
with DAG(... schedule_interval='@daily', catchup=False):
    PythonOperator(
        task_id='run_ibge_pipeline',
        python_callable=run_all_ibge_pipelines
    )
```

## â™»ï¸ IdempotÃªncia com BigQuery
Para evitar duplicaÃ§Ãµes, o projeto usa:
- Tabelas de staging
- Comando MERGE para consolidar dados
- RemoÃ§Ã£o da tabela temporÃ¡ria apÃ³s uso

**BenefÃ­cios**:
- Pode ser reexecutado com seguranÃ§a
- Sem sobrescrever histÃ³ricos
- AtualizaÃ§Ãµes incrementais possÃ­veis

## â–¶ï¸ ExecuÃ§Ã£o Manual
**Local**
```bash
python src/ibge_pipeline/main_ibge.py
python src/bcb_pipeline/main_bcb.py
```

**No Airflow**
- VÃ¡ na UI do Airflow
- Clique em â€œTrigger DAGâ€ â–¶ï¸
- Acompanhe os logs da tarefa

## ğŸ§ª Consultas SQL de ValidaÃ§Ã£o
**IPCA (IBGE)**
```sql
SELECT
  EXTRACT(YEAR FROM data_referencia) AS ano,
  EXTRACT(MONTH FROM data_referencia) AS mes,
  COUNT(*) AS registros,
  AVG(valor_serie) AS media_ipca
FROM `ingestao-dados-publicos.dados_publicos_ibge.ibge_ipca_variacao_mensal_brasil`
GROUP BY ano, mes
ORDER BY ano DESC, mes DESC
LIMIT 10;
```

**SELIC (BCB)**
```sql
SELECT
  data_referencia,
  valor_serie,
  codigo_serie
FROM `ingestao-dados-publicos.dados_publicos_bcb.bcb_selic_diaria`
ORDER BY data_referencia DESC
LIMIT 10;
```

## âœ… Boas PrÃ¡ticas Implementadas
- Estrutura modular por pipeline e funÃ§Ã£o (ETL separado)
- Logging em todos os estÃ¡gios
- Tratamento de exceÃ§Ãµes e alertas
- Uso de `.env` local e `Variable.get()` no Airflow
- Versionamento com `.gitignore` e organizaÃ§Ã£o do projeto
- IdempotÃªncia com BigQuery (MERGE, staging)

## ğŸ“Œ ObservaÃ§Ãµes Finais
**Oportunidades de evoluÃ§Ã£o**:
- ParalelizaÃ§Ã£o por indicador no DAG (via TaskGroup)
- NotificaÃ§Ãµes por Slack ou email em falhas
- IntegraÃ§Ã£o com Looker Studio para dashboards
- Monitoramento com Grafana + Prometheus (ou Cloud Logging)

Engenharia de Dados Moderna nÃ£o Ã© sÃ³ sobre mover dados, mas orquestrar confiabilidade, escalabilidade e rastreabilidade.